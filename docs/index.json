[
{
	"uri": "https://contino.github.io/",
	"title": "Introduction to Kubernetes",
	"tags": [],
	"description": "",
	"content": " Introduction to Kubernetes Introduction to Kubernetes is a hands-on, interactive workshop giving attendees a thorough understanding of the fundamentals of Kubernetes. As part of this workshop, you will learn how Kubernetes works, be able to successfully create a kubernetes cluster, deploy microservices to that cluster \u0026amp; also hear about some war stories.\n"
},
{
	"uri": "https://contino.github.io/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": " Who are you? Agenda Contino Instructors Requirements  "
},
{
	"uri": "https://contino.github.io/introduction/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": " Please Introduce Yourself\u000b  Name Role \u0026amp; Company\u000b What would you like to learn today?  "
},
{
	"uri": "https://contino.github.io/containers/",
	"title": "Containers",
	"tags": [],
	"description": "",
	"content": " History\n Primitives\n Network\n Storage\n Docker\n  "
},
{
	"uri": "https://contino.github.io/kubernetes/",
	"title": "Kubernetes",
	"tags": [],
	"description": "",
	"content": " History Community  CNCF Graduated Projects CNCF Incubating Projects\n Get Involved  Connect to Cluster Kubernetes Objects  Namespaces Pods Resource Quotas Controllers Storage Secrets ConfigMaps Ingress Service Healthchecks  Microservice Exercise Cleanup Options for Running Kubernetes Extras  Logging Monitoring Security   "
},
{
	"uri": "https://contino.github.io/closing/",
	"title": "Closing",
	"tags": [],
	"description": "",
	"content": " Chapter 4 Closing remarks Contact Us:\n|Twitter: @shahadarsh | Twitter: @strongjz | |Email: adarsh.shah@contino.io | Email: james.strong@contino.io |\ncontino.io\n"
},
{
	"uri": "https://contino.github.io/kubernetes/history/",
	"title": "History",
	"tags": [],
	"description": "",
	"content": " Kubernetes History Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.\n“Kubernetes was built to radically change the way that applications are built and deployed in the cloud. Fundamentally, it was designed to give developers more velocity, efficiency, and agility”\nKelsey Hightower, Brendan Burns \u0026amp; Joe Beda -Kubernetes Up and Running Book\n Kubernetes is heavily influenced by Google’s Borg system Released in 2015 when Google partnered with Linux foundation to form CNCF Often called K8s which is a Numeronym K[ubernete]s → K[8]s → K8s Kubernetes - Greek for helmsman or pilot  "
},
{
	"uri": "https://contino.github.io/introduction/agenda/",
	"title": "Agenda",
	"tags": [],
	"description": "",
	"content": " Introduction  Who are you? Agenda Contino Instructors Requirements  Containers  Intro History Primitives Docker Network Storage  Kubernetes  History Community  CNCF Graduated Projects CNCF Incubating Projects\n Get Involved  Connect to Cluster Kubernetes Objects  Namespaces Pods Resource Quotas Controllers Storage Secrets ConfigMaps Ingress Service Healthchecks Cleanup  Options for Running Kubernetes Extras  Monitoring Security    "
},
{
	"uri": "https://contino.github.io/introduction/contino/",
	"title": "Contino",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://contino.github.io/introduction/whoami/",
	"title": "Instructors",
	"tags": [],
	"description": "",
	"content": "   Adarsh Shah James Strong         Technical Principal @ Contino Technical Principal @ Contino   Practice Lead (Cloud Native Soft Dev)    Twitter: @shahadarsh Twitter: @strongjz   Website: https://shahadarsh.com https://www.linkedin.com/in/strongjames/    "
},
{
	"uri": "https://contino.github.io/introduction/requirements/",
	"title": "Requirements",
	"tags": [],
	"description": "",
	"content": " To complete this workshop you will need the following installed locally\nRequired: Docker, git, gcloud, kbectl, gmail account, github account\nOptional: devops debate slack account\nDocker Windows  Directions\n Installer\n  Mac  Directions Installer   gcloud Cloud SDK requires Python 2 with a release version of Python 2.7.9 or later. The installer will install all necessary dependencies, including the needed Python version, by default. If you already have Python 2.x.y installed and want to use the existing installation, you can uncheck the option to install Bundled Python. Note: As of Cloud SDK version 206.0.0, the gcloud CLI has experimental support for running using a Python 3.4+ interpreter (run gcloud topic startup for exclusions and more information on configuring your Python interpreter). All other Cloud SDK tools still require a Python 2.7 interpreter. After installation has completed, accept the following options:\nWindows  Download the Cloud SDK installer. The installer is signed by Google Inc.\n Launch the installer and follow the prompts.\n After installation has completed, accept the following options:\nStart Cloud SDK Shell Run gcloud init\n  Mac  Double Check Python 2 is installed.  python -V   Download one of the following:\nmacOS 64-bit\nmacOS 32-bit\nExtract the contents of the file to any location on your file system.\n Install with this script\n  ./google-cloud-sdk/install.sh   Initialize the install. Open a new terminal so that the changes take effect. Run gcloud init to initialize the SDK:  ./google-cloud-sdk/bin/gcloud init  kubectl You can install kubectl as part of the Google Cloud SDK.\nRun the kubectl installation command:\ngcloud components install kubectl  Test to ensure the version you installed is sufficiently up-to-date:\nkubectl version  Git Download git client from Git Scm\nSlack We will be using to ask/answer questions during and after the workshop. Other DevOps engineers are members of this slack community as well\nDevops Debate account\n"
},
{
	"uri": "https://contino.github.io/introduction/requirements/github/",
	"title": "Exercises",
	"tags": [],
	"description": "",
	"content": " To complete this workshop you will need these repos cloned locally\nGithub Register an account at github.com and clone these repos\n Docker Exercises Kubernetes Exercises  "
},
{
	"uri": "https://contino.github.io/containers/history/",
	"title": "History",
	"tags": [],
	"description": "",
	"content": " Container History 1. In the beginning 2. Hypervisors 3. Containers "
},
{
	"uri": "https://contino.github.io/containers/history/history/",
	"title": "Beginning",
	"tags": [],
	"description": "",
	"content": " In the beginning "
},
{
	"uri": "https://contino.github.io/containers/history/hyper/",
	"title": "Hypervisor",
	"tags": [],
	"description": "",
	"content": " The Hypervisor "
},
{
	"uri": "https://contino.github.io/containers/history/containers/",
	"title": "Containers",
	"tags": [],
	"description": "",
	"content": " Containers "
},
{
	"uri": "https://contino.github.io/containers/history/layers/",
	"title": "Layers",
	"tags": [],
	"description": "",
	"content": " Container Layers "
},
{
	"uri": "https://contino.github.io/kubernetes/community/",
	"title": "Community",
	"tags": [],
	"description": "",
	"content": " CNCF Landscape "
},
{
	"uri": "https://contino.github.io/kubernetes/community/projects_grad/",
	"title": "Graduated Projects",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://contino.github.io/kubernetes/community/projects_inc/",
	"title": "Incubating Projects",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://contino.github.io/kubernetes/community/involvement/",
	"title": "Involvement",
	"tags": [],
	"description": "",
	"content": " Sigs https://github.com/kubernetes/community/blob/master/sig-list.md\n Stackoverflow https://stackoverflow.com/questions/tagged/kubernetes\n Slack\n k8 http://slack.k8s.io/ CNCF https://slack.cncf.io/   "
},
{
	"uri": "https://contino.github.io/containers/primitives/",
	"title": "Primitives",
	"tags": [],
	"description": "",
	"content": " Container Primitives Control Groups Abbreviated cgroups, is a Linux kernel feature that limits, accounts for, and isolates the resource usage\n CPU memory disk I/O network  Namespaces A feature of the Linux kernel that isolate and virtualize system resources of a collection of processes. Examples of resources that can be virtualized include:\n process IDs hostnames user IDs network access interprocess communication filesystems  "
},
{
	"uri": "https://contino.github.io/kubernetes/arch/",
	"title": "Architecture",
	"tags": [],
	"description": "",
	"content": " High level Architecture "
},
{
	"uri": "https://contino.github.io/kubernetes/arch/master/",
	"title": "Master",
	"tags": [],
	"description": "",
	"content": " API data store: Etcd (Cluster State)\n Controller Managers :\n Node Controller Deployment Controller ReplicaSet Controller Replication Controller Endpoints Controller Service Account \u0026amp; Token Controller   Scheduler: Bind pod to Node\n  "
},
{
	"uri": "https://contino.github.io/kubernetes/arch/node/",
	"title": "Node",
	"tags": [],
	"description": "",
	"content": " Kubelet:\n cAdvisor (metrics, logs\u0026hellip;)  Container Runtime:\n docker containerd  Pod:\n Container (one or more)  Kube-proxy:\n Used to reach services and allow communication between Nodes.   "
},
{
	"uri": "https://contino.github.io/kubernetes/arch/data/",
	"title": "Data Flow",
	"tags": [],
	"description": "",
	"content": " Data Flow  CNI: Network Plugin in Kubelet that allows to talk to networking to get IPs for Pods and Services.\n gRPC: API to communicate API Server to ETCD, Controller Manager and Scheduler\n Kubelet - all K8s nodes have a kubelet that ensures that any pod assigned to it are running and configured in the desired state.\n CRI(Container Runtime Interface) gRPC API compiled in kubelet which allows to kubelet to talk to container runtimes by using gRPC API.\n The Container Runtime provider has to adapt it to CRI API to allow kubelet to talk to them by using OCI Standard (runc) Initially, Kubernetes was built on top of Docker as the container runtime. Soon after, CoreOS announced the rkt container runtime and wanted Kubernetes to support it, as well. So, Kubernetes ended up supporting Docker and rkt, although this model wasn\u0026rsquo;t very scalable in terms of adding new features or support for new container runtimes.\n CRI consists of a protocol buffers and gRPC API, and libraries,\n  "
},
{
	"uri": "https://contino.github.io/containers/network/",
	"title": "Network",
	"tags": [],
	"description": "",
	"content": " Container Networking  Docker uses iptables to provide network isolation Explicitly publish a port for connectivity to it Containers do not have a public IPv4 address They are allocated a private address Services running on a container must be exposed port by port Container ports have to be mapped to the host port to avoid conflicts  More information here\n"
},
{
	"uri": "https://contino.github.io/kubernetes/connect/",
	"title": "Connect",
	"tags": [],
	"description": "",
	"content": " Connecting to your GKE Cluster Prerequisites Follow the Requirements\nGoogle SDK Setup Follow the verification steps\ngcloud auth login  Configure SDK ./gcloud-setup.sh  Retrieve Cluster Credentials (make sure there\u0026rsquo;s no conflict with a pre-existing KUBECONFIG var) gcloud container clusters get-credentials \u0026lt;cluster-name\u0026gt; --region \u0026lt;region\u0026gt;  Confirm Cluster Connectivity kubectl cluster-info  "
},
{
	"uri": "https://contino.github.io/containers/network/dockernetop/",
	"title": "Docker Network options",
	"tags": [],
	"description": "",
	"content": " Docker Networking options    Flag value Description     -p 8080:80 Map TCP port 80 in the container to port 8080 on the Docker host.   -p 192.168.1.100:8080:80 Map TCP port 80 in the container to port 8080 on the Docker host for connections to host IP 192.168.1.100.   -p 8080:80/udp Map UDP port 80 in the container to port 8080 on the Docker host.   -p 8080:80/tcp -p 8080:80/udp Map TCP port 80 in the container to TCP port 8080 on the Docker host, and map UDP port 80 in the container to UDP port 8080 on the Docker host.    "
},
{
	"uri": "https://contino.github.io/containers/network/docker0/",
	"title": "Docker0 ",
	"tags": [],
	"description": "",
	"content": " Container Networking  When Docker starts, it creates a virtual interface called docker0 on the host machine docker0 is assigned a random IP address and subnet from the private range defined by RFC 1918 It passes or switches packets between two connected devices just like a physical bridge or switch  Host to container Container to container  Each new container gets one interface that is automatically attached to the docker0 bridge  "
},
{
	"uri": "https://contino.github.io/containers/network/docker0-dig/",
	"title": "Docker0 Diagram",
	"tags": [],
	"description": "",
	"content": " Container Networking "
},
{
	"uri": "https://contino.github.io/containers/storage/",
	"title": "Storage",
	"tags": [],
	"description": "",
	"content": " Storage Layers Volumes Benefits of Volumes Docker Volume command "
},
{
	"uri": "https://contino.github.io/kubernetes/objects/",
	"title": "Objects",
	"tags": [],
	"description": "",
	"content": " Kubernetes API Objects  Namespaces Pods Resource Quotas Controllers Storage Secrets ConfigMaps Ingress Service Healthchecks  "
},
{
	"uri": "https://contino.github.io/containers/storage/layers/",
	"title": "Layers",
	"tags": [],
	"description": "",
	"content": " Layers  An image is a collection of files and some meta data Images are comprised of multiple layers A layer is also just another image Each image contains software you want to run Every image contains a base layer Docker uses a copy on write system Layers are read only  "
},
{
	"uri": "https://contino.github.io/containers/storage/volumes/",
	"title": "Volumes",
	"tags": [],
	"description": "",
	"content": " Volumes A Volume is a designated directory in a container, which is designed to persist data, independent of the container’s life cycle\n Volume changes are excluded when updating an image Persist when a container is deleted Can be mapped to a host folder Can be shared between containers  "
},
{
	"uri": "https://contino.github.io/containers/storage/benefits/",
	"title": "Storage",
	"tags": [],
	"description": "",
	"content": " Benefits of Volumes  De-couple the data that is stored, from the container which created the data Good for sharing data between containers Can setup a data containers which has a volume you mount in other containers Share directories between multiple containers Bypassing the copy on write system to achieve native disk I/O performance Share a host directory with a container Share a single file between the host and container  "
},
{
	"uri": "https://contino.github.io/containers/storage/commands/",
	"title": "Volume CLI",
	"tags": [],
	"description": "",
	"content": " Docker Volume command The docker volume command contains a number of sub commands used to create and manage volumes Commands are\n docker volume create docker volume ls docker volume inspect docker volume rm  "
},
{
	"uri": "https://contino.github.io/kubernetes/objects/namespaces/",
	"title": "Namespaces",
	"tags": [],
	"description": "",
	"content": " Namespaces What Namespaces are virtual clusters inside your Kubernetes cluster that provide logically isolation (kinda) from each other.\nWhy Scope of names Organization of Kubernetes resources\n "
},
{
	"uri": "https://contino.github.io/kubernetes/objects/namespaces/exercises/",
	"title": "Exercises",
	"tags": [],
	"description": "",
	"content": "  Kubernetes yaml files   createNamespace.yaml  (0 ko)    Update createNamespace.yaml with your namespace\nCreate your namespace\nkubectl create -f ./createNamespace.yaml  Verify your namespace exists\n$ kubectl get namespaces NAME STATUS AGE default Active 1d kube-system Active 1d kube-public Active 1d YOUR_NAME_SPACE Active 1d  Set your namespace to the default\nkubectl config set-context $(kubectl config current-context) --namespace=\u0026lt;insert-namespace-name-here\u0026gt;  Validate it\nkubectl config view | grep namespace:  "
},
{
	"uri": "https://contino.github.io/kubernetes/objects/pods/",
	"title": "Pods",
	"tags": [],
	"description": "",
	"content": " Pods Pods are a collection of containers that share a namespace, are colocated and scheduled together on Kubenetes nodes.\nA pod is a group of one or more containers, with shared storage/network, and a specification for how to run the containers\n"
},
{
	"uri": "https://contino.github.io/kubernetes/objects/pods/exercises/",
	"title": "Exercises",
	"tags": [],
	"description": "",
	"content": " Kubernetes github repo\nStart a pod from a manifest kubectl apply -f single-pod-nginx.yaml  Verify the state of the pod kubectl get pods NAME READY STATUS RESTARTS AGE nginx 1/1 Running 0 44s  Get more details of the pod kubectl describe pod nginx  Accessing your pod kubectl port-forward nginx 8080:80  Logs kubectl logs -f nginx  Navigate to http://localhost:8080 in your web browser\nMulti container pod Start a pod from a manifest kubectl apply -f multi-pod.yaml  kubectl describe pod multi-pod  Accessing your pod kubectl port-forward multi-pod 8080:80  Navigate to http://localhost:8080/date.log in your web browser\nCleanup kubectl delete -f single-pod-nginx.yaml kubectl delete -f multi-pod.yaml  "
},
{
	"uri": "https://contino.github.io/kubernetes/objects/pods/labels_selectors/",
	"title": "Labels and Selectors",
	"tags": [],
	"description": "",
	"content": " Labels Labels are key/value pairs that are attached to objects, such as pods that help to identify that object.\nSelectors Label Selectors help client/user identify a set of objects.\nspec: selector: matchLabels: app: mysql strategy: type: Recreate template: metadata: labels: app: mysql  Demo Create labels \u0026amp; use selector to identify set of objects\n"
},
{
	"uri": "https://contino.github.io/kubernetes/objects/resourcequotas/",
	"title": "Resource Quotas",
	"tags": [],
	"description": "",
	"content": " What Requests - How much does this pod need to run\nLimits - This pod only gets this much to run\nWhy Kubernetes being a multi-tenant environment, some applications may hog resources and starve others, Resource Qoatas discourage this behavior\n "
},
{
	"uri": "https://contino.github.io/kubernetes/objects/resourcequotas/resources/",
	"title": "Resource Quotas",
	"tags": [],
	"description": "",
	"content": " Compute Resources  CPU Memory Storage  requests.storage persistentvolumeclaims storage-class-name.storageclass.storage.k8s.io/requests.storage storage-class-name.storageclass.storage.k8s.io/persistentvolumeclaims  Object Count\n configmaps persistentvolumeclaims pods replicationcontrollers resourcequotas services services.loadbalancers services.nodeports secrets  Priority - low, medium, high\n  More info\n"
},
{
	"uri": "https://contino.github.io/kubernetes/objects/resourcequotas/exercises/",
	"title": "Exercises",
	"tags": [],
	"description": "",
	"content": "   Kubernetes yaml files   pod-limits.yaml  (0 ko)   pod-no-quotas.yaml  (0 ko)   pod-only-limit.yaml  (0 ko)   pod-only-request.yaml  (0 ko)   quotas.yaml  (0 ko)    Deploy default resources quotas for your namespace\nkubectl create -f quotas.yaml kubectl describe quota  Verify qoutas\nkubectl get resourcequota mem-cpu-rq --output=yaml  Deploy specific limits and requests for pods\nkubectl create -f pod-limits.yaml kubectl describe limits mem-limit-range  Deploy pods with only limits\nkubectl create -f pod-only-limit.yaml Error from server (Forbidden): error when creating \u0026quot;pod-only-limit.yaml\u0026quot;: pods \u0026quot;only-limits\u0026quot; is forbidden: failed quota: mem-cpu-rq: must specify limits.cpu,requests.cpu  Deploy pods with only requests\nkubectl create -f pod-only-request.yaml Error from server (Forbidden): error when creating \u0026quot;pod-only-request.yaml\u0026quot;: pods \u0026quot;only-requests\u0026quot; is forbidden: failed quota: mem-cpu-rq: must specify limits.cpu,requests.cpu  Deploy pod with neither and see defaults applied from the namespace\nkubectl create -f pod-no-quotas.yaml Error from server (Forbidden): error when creating \u0026quot;pod-no-quotas.yaml\u0026quot;: pods \u0026quot;no-quotas\u0026quot; is forbidden: failed quota: mem-cpu-rq: must specify limits.cpu,requests.cpu  Please Delete Quotas! kubectl delete -f quotas.yaml kubectl delete -f pod-limits.yaml  "
},
{
	"uri": "https://contino.github.io/kubernetes/objects/controllers/",
	"title": "Controllers",
	"tags": [],
	"description": "",
	"content": " Controllers In Kubernetes, a controller is a control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state. There are several in the Kubernetes Architecture that support different functions in the system.\nNamespace controller - Creates and updates the Namespaces in kubernetes\nServiceaccounts controller - Manages the service accounts in the system, which are for processes to interact with Kubernetes.\nNode Controller - Responsible for noticing and responding when nodes go down. Service Account \u0026amp; Token Controllers: Create default accounts and API access tokens for new namespaces.\nDeployment Controller - A Deployment controller provides declarative updates for Pods and ReplicaSets.\nReplication Controller - Responsible for maintaining the correct number of pods for every replication controller object in the system.\nEndpoints Controller - Populates the Endpoints object (that is, joins Services \u0026amp; Pods). When services are created, the Endpoint controller manages the connection between services and the pods back the service.\n"
},
{
	"uri": "https://contino.github.io/kubernetes/objects/controllers/updates/",
	"title": "Controllers Updates",
	"tags": [],
	"description": "",
	"content": " Deployments  Scaling Rolling\u000b  ReplicaSet  Desired state  Strategies:  Recreate RollingUpdate (default) Blue/Green Canary A/B Testing  "
},
{
	"uri": "https://contino.github.io/kubernetes/objects/controllers/exercises/",
	"title": "Exercises",
	"tags": [],
	"description": "",
	"content": " In these exercises we will be working with these controllers, later we will working with Endpoints, since they are directly related to Services.\n  Kubernetes yaml files   deploy-nginx-1.9.1-scale.yaml  (0 ko)   deploy-nginx-1.9.1.yaml  (0 ko)   deploy-nginx.yaml  (0 ko)   replicaset.yaml  (0 ko)    Deployment Controller - Responsible for the controlled deployment of pods and ReplicaSets.\nReplicaSets - is the newest version of the Replication Controller. The only difference between a ReplicaSet and a Replication Controller right now is the selector support.\nReplicaSets Create the replicaset\nkubectl apply -f replicaset.yaml  Get the status of the replicaset\nkubectl get replicaset  Retrieve detailed information about the replicaset\nkubectl describe replicaset frontend  See the status of pods deployed with replicaset\nkubectl get pods -l tier: frontend  Select one of the pods from the ReplicaSet\nkubect port-forward \u0026lt;ONE_POD_FROM_REPLICASET\u0026gt; 8080:80  In another terminal window run\nkubectl logs -f \u0026lt;POD_FROM_REPLICASET\u0026gt;  Go to http://localhost:8080/\nYou should see logs streaming to the console\nScale a ReplicaSet\nkubectl scale --replicas=4 rs/frontend  The replicas will run 4 pods now\nkubectl describe rs/frontend  kubectl get pods -l tier: frontend  Deployment Controller Create the nginx deployment\nkubectl create -f deploy-nginx.yaml  Get deployment information\nkubectl get deployments  Where is the deployment\nkubectl rollout status deployment/nginx-deployment  Update the deployments\nkubectl apply -f deploy-nginx-1.9.1.yaml  Where is the deployment\nkubectl rollout status deployment/nginx-deployment  History of deployment\nkubectl get replicaset  View the pods in the deployment\nkubectl get pods  Get more detailed information about the deployment\nkubectl describe deployment nginx-deployment  Scale the deployment\nkubectl apply -f deploy-nginx-1.9.1-scale.yaml  Cleanup kubectl delete -f replicaset.yaml kubectl delete -f deploy-nginx.yaml  "
},
{
	"uri": "https://contino.github.io/kubernetes/objects/storage/",
	"title": "Storage",
	"tags": [],
	"description": "",
	"content": " What Storage like compute is another resource that must be managed. Kubernetes offers 3 types of storage\n Volumes Persistent Volumes Persistent Volume Claims  Why The Ephemeral nature of pods and containers lead to the need for data to be have a decoupled lifecycle outside of containers and pods.\n "
},
{
	"uri": "https://contino.github.io/kubernetes/objects/storage/volumes/",
	"title": "Volumes",
	"tags": [],
	"description": "",
	"content": "Several volume types are supported\n awsElasticBlockStore azureDisk gcePersistentDisk hostPath secret configmaps  awsElasticBlockStore example yaml:\napiVersion: v1 kind: Pod metadata: name: test-ebs spec: containers: - image: k8s.gcr.io/test-webserver name: test-container volumeMounts: - mountPath: /test-ebs name: test-volume volumes: - name: test-volume # This AWS EBS volume must already exist. awsElasticBlockStore: volumeID: \u0026lt;volume-id\u0026gt; fsType: ext4  "
},
{
	"uri": "https://contino.github.io/kubernetes/objects/storage/pv/",
	"title": "Persistent Volume",
	"tags": [],
	"description": "",
	"content": "Persistent Volumes (PV\u0026rsquo;s) are a piece of storage provisioned in a cluster and can be used/reference in the cluster like another other resource.\nProvisioning - Static or Dynamic\nTypes of PV\u0026rsquo;s\n GCEPersistentDisk AWSElasticBlockStore AzureFile CephFS  kind: PersistentVolume apiVersion: v1 metadata: name: task-pv-volume labels: type: local spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \u0026quot;/mnt/data\u0026quot;  "
},
{
	"uri": "https://contino.github.io/kubernetes/objects/storage/pvc/",
	"title": "Persistent Volume Claims",
	"tags": [],
	"description": "",
	"content": "Persistent Volume Claims (PVC\u0026rsquo;s) - Allow pods to requests and attache Persistent Volumes available in the cluster.\nWhen used in with Dynamic provision and Storage Classes, PVC\u0026rsquo;s can automatically make storage available on demand.\nTypes of PVC\u0026rsquo;s\n GCEPersistentDisk AWSElasticBlockStore AzureFile CephFS  kind: Pod apiVersion: v1 metadata: name: mypod spec: containers: - name: myfrontend image: nginx volumeMounts: - mountPath: \u0026quot;/var/www/html\u0026quot; name: mypd volumes: - name: mypd persistentVolumeClaim: claimName: myclaim  "
},
{
	"uri": "https://contino.github.io/kubernetes/objects/storage/classes/",
	"title": "Storage Classes",
	"tags": [],
	"description": "",
	"content": "Storage classes allow cluster administrators to provide varing levels of support and types of storage to applications in a cluster\nExample: Storage class that will provision an AWS EBS Volumes when referenced a PVC\nkind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: standard provisioner: kubernetes.io/aws-ebs parameters: type: gp2 reclaimPolicy: Retain mountOptions: - debug volumeBindingMode: Immediate  "
},
{
	"uri": "https://contino.github.io/kubernetes/objects/storage/exercises/",
	"title": "Exercises",
	"tags": [],
	"description": "",
	"content": "   Kubernetes yaml files   mysql-pod.yaml  (0 ko)   mysql-pv.yaml  (0 ko)    Create the persistent and the claim\nkubectl apply -f mysql-pv.yaml  Create a pod that will use it.\nkubectl apply -f mysql-pod.yaml  Clean up kubectl delete -f mysql-pv.yaml kubectl delete -f mysql-pod.yaml  "
},
{
	"uri": "https://contino.github.io/kubernetes/objects/secrets/",
	"title": "Secrets",
	"tags": [],
	"description": "",
	"content": " What Kubernetes object to inject sensitive data into containers\nWhy Sensitive data should never be built into the container image, in order to do that, kubernetes offers Secrets.\n "
},
{
	"uri": "https://contino.github.io/kubernetes/objects/secrets/secret/",
	"title": "Secrets Uses - Secret",
	"tags": [],
	"description": "",
	"content": "echo -n 'admin' | base64 YWRtaW4= echo -n '1f2d1e2e67df' | base64 MWYyZDFlMmU2N2Rm  Write a Secret that looks like this:\napiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: username: YWRtaW4= password: MWYyZDFlMmU2N2Rm  "
},
{
	"uri": "https://contino.github.io/kubernetes/objects/secrets/file/",
	"title": "Secrets Uses - File",
	"tags": [],
	"description": "",
	"content": "apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: redis volumeMounts: - name: foo mountPath: \u0026quot;/etc/foo\u0026quot; readOnly: true volumes: - name: foo secret: secretName: mysecret  "
},
{
	"uri": "https://contino.github.io/kubernetes/objects/secrets/env/",
	"title": "Secrets Uses - Env Var",
	"tags": [],
	"description": "",
	"content": "apiVersion: v1 kind: Pod metadata: name: secret-env-pod spec: containers: - name: mycontainer image: redis env: - name: SECRET_USERNAME valueFrom: secretKeyRef: name: mysecret key: username - name: SECRET_PASSWORD valueFrom: secretKeyRef: name: mysecret key: password restartPolicy: Never  "
},
{
	"uri": "https://contino.github.io/kubernetes/objects/configmaps/",
	"title": "Configmaps",
	"tags": [],
	"description": "",
	"content": " Configmaps What Why  "
},
{
	"uri": "https://contino.github.io/kubernetes/objects/ingress/",
	"title": "ingress",
	"tags": [],
	"description": "",
	"content": " What Ingress is a K8 object that allows external access to resources inside the cluster\nWhy Services, Pods and other objects are only accessible inside the cluster\n "
},
{
	"uri": "https://contino.github.io/kubernetes/objects/ingress/controller/",
	"title": "Ingress Controllers",
	"tags": [],
	"description": "",
	"content": " Ingress controllers In order for the ingress resource to work, the cluster must have an ingress controller running. This is unlike other types of controllers, which run as part of the kube-controller-manager binary, and are typically started automatically with a cluster. Choose the ingress controller implementation that best fits your cluster.\nIngress Controller\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: test-ingress spec: backend: serviceName: testsvc servicePort: 80  Kubernetes supported Ingress Controllers:\n GCE nginx  Others that can be deployed:\n HAProxy Kong Contour  Full list is here\n"
},
{
	"uri": "https://contino.github.io/kubernetes/objects/ingress/rules/",
	"title": "Ingress Rules",
	"tags": [],
	"description": "",
	"content": "Each http rule contains the following information:\n Host list of paths Backend service  Ingress Rule\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: simple-fanout-example annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: foo.bar.com http: paths: - path: /foo backend: serviceName: service1 servicePort: 4200 - path: /bar backend: serviceName: service2 servicePort: 8080  "
},
{
	"uri": "https://contino.github.io/kubernetes/objects/services/",
	"title": "Services",
	"tags": [],
	"description": "",
	"content": " What Service: a named abstraction of software service, consisting of a port that the proxy listens on, and the selector that determines which pods will answer requests.\nWhy Pods come and go, and with that their IP address change rapidly. Services decouple the IP address from the application and serve as the IP address inside the cluster for an application running multiple pods.\nMore info here\n "
},
{
	"uri": "https://contino.github.io/kubernetes/objects/services/diag/",
	"title": "Service Daigram",
	"tags": [],
	"description": "",
	"content": "  "
},
{
	"uri": "https://contino.github.io/kubernetes/objects/services/exercises/",
	"title": "Exercises",
	"tags": [],
	"description": "",
	"content": "   Kubernetes yaml files   app.yaml  (1 ko)   mysql-all.yaml  (1 ko)   mysql-service.yaml  (0 ko)    Pod Deployment with health checks, PersistentVolume and claim Since we have created the mysql pod several times, here is a yaml file that creates it all.\nCreate a secret for the password between Wordpress and MYSQL kubectl create secret generic mysql-pass --from-literal=password=YOUR_PASSWORD  Verify it is there\nkubectl get secrets  Deploy mysql\nkubectl apply -f mysql-all.yaml  Verify mysql deployed properly\nkubectl get deploy  Services Deploy the service for mysql\nkubectl apply -f mysql-service.yaml  Verify the service has endpoints.\nkubectl get services -o wide  Application Deployment deploy the application that will use mysqld\nkubectl apply -f app.yaml  Verify Service\nkubectl get services wordpress  Clean up kubectl delete -f mysql-all.yaml kubectl delete -f mysql-service.yaml kubectl delete -f app.yaml  "
},
{
	"uri": "https://contino.github.io/kubernetes/objects/healthchecks/",
	"title": "Healthchecks",
	"tags": [],
	"description": "",
	"content": " What Healthchecks inform that kubelet that pods are ready to accept traffic\nWhy The distributed nature of kubernetes allows pods to come and go for a number of reasons, and if many are running a application the kubelet needs to know what a \u0026ldquo;healthy\u0026rdquo; pod looks like.\n"
},
{
	"uri": "https://contino.github.io/kubernetes/objects/daemonsets/",
	"title": "Daemonsets",
	"tags": [],
	"description": "",
	"content": " Daemonsets What Specialized deployments that will deploy pods on every node in the cluster\nWhy  running a cluster storage daemon, such as glusterd, ceph, on each node. running a logs collection daemon on every node, such as fluentd or logstash. running a node monitoring daemon on every node,  Prometheus Node Exporter collectd Dynatrace OneAgent AppDynamics Agent Datadog agent     "
},
{
	"uri": "https://contino.github.io/kubernetes/objects/healthchecks/liveliness/",
	"title": "Liveliness",
	"tags": [],
	"description": "",
	"content": " Readiness and Liveliness Liveliness Liveliness checks inform the kubelet that the pod is running. If this check fails the kubelet will attempt to restart the pod.\napiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-exec spec: containers: - name: liveness image: k8s.gcr.io/busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5  "
},
{
	"uri": "https://contino.github.io/kubernetes/objects/healthchecks/readiness/",
	"title": "Readiness",
	"tags": [],
	"description": "",
	"content": " Readiness and Liveliness Readiness Readiness checks let the kubelet know that the pod is ready to receive traffic. For example if this check fails the Service or Load balancer does send traffic to that pod.\nreadinessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5  "
},
{
	"uri": "https://contino.github.io/kubernetes/objects/healthchecks/exercises/",
	"title": "Exercises",
	"tags": [],
	"description": "",
	"content": " Exercises   Kubernetes yaml files   mysql-config.yaml  (0 ko)   mysql-health.yaml  (1 ko)   mysql-pv.yaml  (0 ko)    If you deleted the PV from the previous exercise please recreate it\nkubectl apply -f mysql-pv.yaml  Mysql conf needed for testing\nkubectl create -f mysql-config.yaml  Start the Mysql deployment\nkubectl apply -f mysql-health.yaml  Get the pod name\nPOD_NAME=$(kubectl get pods -l app=mysql -o=jsonpath='{.items[0].metadata.name}')  Describe pod should display a healthy pod\nkubectl describe pod $POD_NAME  Breaking the readiness probe\nkubectl exec $POD_NAME -c mysql -- mv /usr/bin/mysql /usr/bin/mysql.off  Check to see if the pods is running\nkubectl get pods -l app=mysql  Output if the pod is not running\nNAME READY STATUS RESTARTS AGE mysql-6b98cc4475-xgwkp 0/1 Running 0 6m  Get more details about the pod\nkubectl describe pod $POD_NAME  Output from the error\nReadiness probe failed: OCI runtime exec failed: exec failed: container_linux.go:348: starting container process caused \u0026quot;exec: \\\u0026quot;/usr/bin/mysql\\\u0026quot;: stat /usr/bin/mysql: no such file or directory\u0026quot;: unknown  Fix the readiness probe\nkubectl exec $POD_NAME -c mysql -- mv /usr/bin/mysql.off /usr/bin/mysql  Output now that the pods is running again\nNAME READY STATUS RESTARTS AGE mysql-6b98cc4475-xgwkp 1/1 Running 0 6m  "
},
{
	"uri": "https://contino.github.io/containers/docker/",
	"title": "Docker Intro",
	"tags": [],
	"description": "",
	"content": " Docker Benefits Platform Dockerfile Docker cli Docker Exercise "
},
{
	"uri": "https://contino.github.io/kubernetes/micro/",
	"title": "Microservices Exercise",
	"tags": [],
	"description": "",
	"content": "Git clone Exercises\n"
},
{
	"uri": "https://contino.github.io/containers/docker/benefits/",
	"title": "Benefits of Docker",
	"tags": [],
	"description": "",
	"content": " Docker Benefits  Separation of concerns Developers focus on building their apps System admins focus on deployment Fast development cycle Application portability Build in one environment, ship to another Scalability Easily spin up new containers if needed Run more apps on one host machine  "
},
{
	"uri": "https://contino.github.io/containers/docker/platform/",
	"title": "Platform",
	"tags": [],
	"description": "",
	"content": " Docker "
},
{
	"uri": "https://contino.github.io/containers/docker/dockerfile/",
	"title": "Dockerfile",
	"tags": [],
	"description": "",
	"content": " Docker Dockerfile  Instructions specify what to do when building the image FROM instruction specifies what the base image should be RUN instruction specifies a command to execute Comments start with “#” Remember, each line in a Dockerfile creates a new layer if it changes the state of the image You need to find the right balance between having lots of layers created for the image and readability of the Dockerfile Don’t install unnecessary packages One ENTRYPOINT per Dockerfile Combine similar commands into one by using “\u0026amp;\u0026amp;” and “\\”\n Use the caching system to your advantage The order of statements is important Add files that are least likely to change first and the ones most likely to change last  "
},
{
	"uri": "https://contino.github.io/containers/docker/cli/",
	"title": "CLI",
	"tags": [],
	"description": "",
	"content": " Docker Docker cli  docker ps docker images docker logs docker exec  Official Documentation\nMore exercises\n"
},
{
	"uri": "https://contino.github.io/containers/docker/exercise/",
	"title": "Docker Exercise",
	"tags": [],
	"description": "",
	"content": " Docker Exercise Git clone Docker Exercises\n"
},
{
	"uri": "https://contino.github.io/kubernetes/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": " Deleting all resources used All in one go\nkubectl delete po,svc,pv,deploy,rs,qouta,namespace,configmaps,secrets,ing,daemonsets --all  Pods and Services\nkubectl delete po,svc --all  PersistentVolume\nkubectl delete pv --all  Deployments\nkubectl delete deploy --all  Replicaset\nkubectl delete rs --all  Resource quotas\nkubectl delete quota --all  "
},
{
	"uri": "https://contino.github.io/kubernetes/running/",
	"title": "Running",
	"tags": [],
	"description": "",
	"content": " Options for Running Kubernetes "
},
{
	"uri": "https://contino.github.io/kubernetes/extras/",
	"title": "Extras",
	"tags": [],
	"description": "",
	"content": " Logging Monitoring Security  "
},
{
	"uri": "https://contino.github.io/kubernetes/extras/monitoring/",
	"title": "Monitoring",
	"tags": [],
	"description": "",
	"content": " Monitoring "
},
{
	"uri": "https://contino.github.io/kubernetes/extras/logging/",
	"title": "Logging",
	"tags": [],
	"description": "",
	"content": " kubectl logs Node level Cluster Level Side Car  "
},
{
	"uri": "https://contino.github.io/kubernetes/extras/logging-kc/",
	"title": "Logging - Pod",
	"tags": [],
	"description": "",
	"content": " Logging  kubectl logs  apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: [/bin/sh, -c, 'i=0; while true; do echo \u0026quot;$i: $(date)\u0026quot;; i=$((i+1)); sleep 1; done']  $ kubectl logs counter 0: Mon Jan 1 00:00:00 UTC 2001 1: Mon Jan 1 00:00:01 UTC 2001 2: Mon Jan 1 00:00:02 UTC 2001  "
},
{
	"uri": "https://contino.github.io/kubernetes/extras/logging-node/",
	"title": "Logging - Node",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://contino.github.io/kubernetes/extras/logging-cluster/",
	"title": "Logging - Cluster",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://contino.github.io/kubernetes/extras/security/",
	"title": "Security - Containers",
	"tags": [],
	"description": "",
	"content": " Container security primitives  SElinux AppArmor Seccomp https://docs.docker.com/engine/security/seccomp/#run-without-the-default-seccomp-profile  Container Pipeline  Establish a pipeline to build a standard image Have a versioning policy Allow to only run images based of the standard image Use the same OS as the host Keep the image small Use a private registry Don’t embed secrets into images, use Hashicorp Vault https://www.cisecurity.org/benchmark/docker/ https://github.com/docker/docker-bench-security  "
},
{
	"uri": "https://contino.github.io/kubernetes/extras/security1/",
	"title": "Security - K8",
	"tags": [],
	"description": "",
	"content": " K8 Security  RBAC NetworkPolicy TLS Image Scanning Aquasec/Twistlock Integrating with HashiCorp Vault other public cloud secret stores Investigate using a container based OS (CoreOS, Atomic Linux) Harden and tweak Make sure to pass https://github.com/dev-sec/linux-baseline  Vendors to add  Aquasec https://www.aquasec.com/ Twistlock https://www.twistlock.com/ Sysdig Falco https://www.sysdig.org/falco/  "
},
{
	"uri": "https://contino.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://contino.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]